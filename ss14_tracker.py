"""
SS14 è‡ªåŠ¨åŒ–æ±‰åŒ–å·¥å…·ç®± v3.2
==========================
æ”¯æŒ GUI å’Œå‘½ä»¤è¡Œä¸¤ç§æ¨¡å¼ï¼Œé›†æˆæå–ã€åŒæ­¥ã€åˆå¹¶åŠŸèƒ½ã€‚

ä¼˜åŒ–ç‰¹æ€§:
- ä¸€é”®å·¥ä½œæµ
- ç›®å½•è‡ªåŠ¨æ£€æµ‹
- è¿›åº¦æ¡
- API è¯·æ±‚é‡è¯•
- å¯é…ç½®å­—æ®µåˆ—è¡¨
- é«˜DPIå±å¹•æ”¯æŒ
- å¢é‡æå–æ¨¡å¼
"""

import os
import sys
import json
import argparse
import subprocess
import threading
import io
import time
import hashlib
import requests
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable, Set

# ==================== é«˜ DPI æ”¯æŒ (Windows) ====================
# åœ¨å¯¼å…¥ tkinter ä¹‹å‰è®¾ç½® DPI æ„ŸçŸ¥ï¼Œè§£å†³ 2K/4K å±å¹•å­—ä½“æ¨¡ç³Šé—®é¢˜

def enable_high_dpi():
    """å¯ç”¨ Windows é«˜ DPI æ”¯æŒ"""
    if sys.platform == 'win32':
        try:
            from ctypes import windll
            # è®¾ç½®è¿›ç¨‹çº§åˆ«çš„ DPI æ„ŸçŸ¥
            # PROCESS_PER_MONITOR_DPI_AWARE = 2
            windll.shcore.SetProcessDpiAwareness(2)
        except Exception:
            try:
                # å›é€€åˆ°æ—§ç‰ˆ API
                windll.user32.SetProcessDPIAware()
            except Exception:
                pass

# åœ¨åŠ è½½ GUI å‰è°ƒç”¨
enable_high_dpi()

# ==================== å¸¸é‡é…ç½® (Constants) ====================

# é»˜è®¤å¯ç¿»è¯‘å­—æ®µåˆ—è¡¨ï¼ˆå¯åœ¨é…ç½®ä¸­ä¿®æ”¹ï¼‰
DEFAULT_TRANSLATABLE_FIELDS = ['name', 'description']

# API é‡è¯•é…ç½®
API_RETRY_COUNT = 3
API_RETRY_DELAY = 2  # ç§’

# é…ç½®æ–‡ä»¶å
CONFIG_FILE = "config.json"

# æå–è¾“å‡ºç›®å½•ï¼ˆç”¨äºæŒ‰æ–‡ä»¶å¤¹åˆ†ç»„æ¨¡å¼ï¼‰
EXTRACT_OUTPUT_DIR = "extracted"

# ==================== FTL é”®æ£€æµ‹ (FTL Key Detection) ====================

def is_ftl_key(text: str) -> bool:
    """
    æ£€æµ‹å­—ç¬¦ä¸²æ˜¯å¦ä¸º FTL æœ¬åœ°åŒ–é”®ã€‚
    
    FTL é”®ç‰¹å¾ï¼š
    - å…¨å°å†™
    - åŒ…å«è¿å­—ç¬¦ -
    - æ ¼å¼å¦‚ "word-word-word"ï¼ˆè‡³å°‘2æ®µç”¨è¿å­—ç¬¦è¿æ¥çš„çº¯å­—æ¯è¯ï¼‰
    
    ç¤ºä¾‹ï¼š
    - "loadout-group-weapon" -> True (FTL é”®)
    - "Assault Rifle" -> False (æ­£å¸¸æ–‡æœ¬)
    - "AK-47" -> False (åŒ…å«æ•°å­—å’Œå¤§å†™)
    """
    if not text or not isinstance(text, str):
        return False
    
    text = text.strip()
    
    # å¿…é¡»åŒ…å«è¿å­—ç¬¦
    if '-' not in text:
        return False
    
    # å¿…é¡»å…¨å°å†™ï¼ˆFTL é”®é€šå¸¸å…¨å°å†™ï¼‰
    if text != text.lower():
        return False
    
    # ä¸èƒ½åŒ…å«ç©ºæ ¼ï¼ˆæ­£å¸¸æ–‡æœ¬é€šå¸¸æœ‰ç©ºæ ¼ï¼‰
    if ' ' in text:
        return False
    
    # æ£€æŸ¥æ˜¯å¦ç¬¦åˆ word-word æ¨¡å¼ï¼ˆè‡³å°‘2æ®µï¼‰
    parts = text.split('-')
    if len(parts) < 2:
        return False
    
    # æ¯æ®µéƒ½åº”è¯¥æ˜¯çº¯å­—æ¯ï¼ˆå…è®¸ç©ºæ®µå¦‚ "foo--bar" ä¹Ÿè·³è¿‡ï¼‰
    for part in parts:
        if part and not part.isalpha():
            return False
    
    return True

# ==================== å…±äº«å·¥å…· (Utils) ====================

# å…¨å±€è¿›åº¦å›è°ƒï¼ˆç”¨äº GUI æ›´æ–°è¿›åº¦æ¡ï¼‰
_progress_callback: Optional[Callable[[int, int, str], None]] = None

def set_progress_callback(callback: Optional[Callable[[int, int, str], None]]):
    """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•° (current, total, message)"""
    global _progress_callback
    _progress_callback = callback

def report_progress(current: int, total: int, message: str = ""):
    """æŠ¥å‘Šè¿›åº¦"""
    if _progress_callback:
        _progress_callback(current, total, message)

def log(message: str, level: str = "INFO"):
    """å¸¦æ—¶é—´æˆ³çš„ç®€å•æ—¥å¿—è®°å½•å™¨"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] [{level}] {message}")
    sys.stdout.flush()

def error(message: str):
    """è¾“å‡ºé”™è¯¯å¹¶é€€å‡º"""
    log(message, "ERROR")
    sys.exit(1)

def detect_game_directory() -> Optional[str]:
    """
    è‡ªåŠ¨æ£€æµ‹æ¸¸æˆç›®å½•ã€‚
    æ£€æŸ¥å¸¸è§çš„ SS14 ç›®å½•ç»“æ„ã€‚
    """
    candidates = [
        "Resources/Prototypes",
        "Content/Resources/Prototypes",
        "Resources",
        "Content",
    ]
    
    for candidate in candidates:
        if os.path.isdir(candidate):
            log(f"è‡ªåŠ¨æ£€æµ‹åˆ°æ¸¸æˆç›®å½•: {candidate}")
            return candidate
    
    return None

# ==================== YAML å¤„ç†å™¨ (YAML Processor) ====================

try:
    from ruamel.yaml import YAML
    HAS_RUAMEL = True
except ImportError:
    HAS_RUAMEL = False
    import yaml

class YAMLProcessor:
    """YAML å¤„ç†å™¨ - ä½¿ç”¨ ruamel.yaml ä¿ç•™æ³¨é‡Šã€æ ¼å¼å’Œè‡ªå®šä¹‰æ ‡ç­¾ã€‚"""
    
    def __init__(self):
        if HAS_RUAMEL:
            self._yaml = YAML()
            self._yaml.preserve_quotes = True
            self._yaml.width = 4096  # é˜²æ­¢è‡ªåŠ¨æ¢è¡Œ
            self._yaml.default_flow_style = None  # ä¿ç•™åŸå§‹æµå¼æ ·å¼
            self._yaml.allow_duplicate_keys = True
            self._yaml.indent(mapping=2, sequence=4, offset=2)
        else:
            self._yaml = None
            print("è­¦å‘Š: æœªæ‰¾åˆ° ruamel.yamlã€‚å›é€€åˆ°åŸºç¡€ PyYAMLï¼ˆå°†ä¸¢å¤±æ ¼å¼ï¼‰ã€‚")
    
    def load(self, file_path: str) -> Any:
        """åŠ è½½ YAML æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            if HAS_RUAMEL:
                return self._yaml.load(f)
            else:
                return yaml.safe_load(f)
    
    def dump(self, data: Any, file_path: str):
        """ä¿å­˜ YAML æ–‡ä»¶ï¼ˆä¿ç•™æ ¼å¼ï¼‰"""
        with open(file_path, 'w', encoding='utf-8') as f:
            if HAS_RUAMEL:
                self._yaml.dump(data, f)
            else:
                yaml.dump(data, f, allow_unicode=True, sort_keys=False, 
                         default_flow_style=False, indent=2)

    def load_from_string(self, content: str) -> Any:
        """ä»å­—ç¬¦ä¸²åŠ è½½ YAML"""
        if HAS_RUAMEL:
            return self._yaml.load(io.StringIO(content))
        else:
            return yaml.safe_load(content)

# ==================== Paratranz å®¢æˆ·ç«¯ (PZ Client) ====================

class PZClient:
    """
    Paratranz API å®¢æˆ·ç«¯
    æ ¹æ®å®˜æ–¹æ–‡æ¡£å®ç°ï¼šhttps://paratranz.cn/docs
    """
    BASE_URL = "https://paratranz.cn/api"

    def __init__(self, project_id: int, token: str):
        self.project_id = project_id
        self.token = token
        # å®˜æ–¹æ–‡æ¡£è¦æ±‚ï¼šAuthorization: Bearer {TOKEN}
        self.headers = {"Authorization": f"Bearer {token}"}
        if not project_id or not token:
            raise ValueError("éœ€è¦æä¾› Project ID å’Œ Token")
        log(f"åˆå§‹åŒ– Paratranz å®¢æˆ·ç«¯: é¡¹ç›®ID={project_id}")

    def _request_with_retry(self, method: str, url: str, **kwargs) -> requests.Response:
        """å¸¦é‡è¯•çš„è¯·æ±‚"""
        last_error = None
        for attempt in range(API_RETRY_COUNT):
            try:
                log(f"å‘é€è¯·æ±‚: {method} {url}")
                response = requests.request(method, url, headers=self.headers, timeout=30, **kwargs)
                
                log(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                
                # å¤„ç†å¸¸è§é”™è¯¯
                if response.status_code == 401:
                    log("Token é”™è¯¯æˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥ä½ çš„ API Token", "ERROR")
                    return response
                
                if response.status_code == 403:
                    log("æ²¡æœ‰æƒé™è®¿é—®è¯¥èµ„æº", "ERROR")
                    return response
                
                if response.status_code == 404:
                    log("èµ„æºä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥é¡¹ç›®IDæ˜¯å¦æ­£ç¡®", "ERROR")
                    return response
                
                # å¤„ç†é€Ÿç‡é™åˆ¶
                if response.status_code == 429:
                    wait_time = int(response.headers.get('Retry-After', API_RETRY_DELAY * 2))
                    log(f"API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...", "WARNING")
                    time.sleep(wait_time)
                    continue
                
                return response
                
            except requests.exceptions.RequestException as e:
                last_error = e
                log(f"è¯·æ±‚å¼‚å¸¸: {e}", "ERROR")
                if attempt < API_RETRY_COUNT - 1:
                    log(f"{API_RETRY_DELAY} ç§’åé‡è¯• ({attempt + 1}/{API_RETRY_COUNT})...", "WARNING")
                    time.sleep(API_RETRY_DELAY)
                    
        raise last_error if last_error else Exception("è¯·æ±‚å¤±è´¥")

    def test_connection(self) -> bool:
        """æµ‹è¯• API è¿æ¥å’Œ Token æœ‰æ•ˆæ€§"""
        log("æ­£åœ¨æµ‹è¯• API è¿æ¥...")
        try:
            url = f"{self.BASE_URL}/projects/{self.project_id}"
            response = self._request_with_retry("GET", url)
            
            if response.status_code == 200:
                data = response.json()
                project_name = data.get('name', 'æœªçŸ¥')
                log(f"âœ… è¿æ¥æˆåŠŸï¼é¡¹ç›®åç§°: {project_name}")
                return True
            elif response.status_code == 401:
                log("âŒ Token æ— æ•ˆæˆ–å·²è¿‡æœŸ", "ERROR")
                return False
            elif response.status_code == 404:
                log(f"âŒ é¡¹ç›® ID {self.project_id} ä¸å­˜åœ¨", "ERROR")
                return False
            else:
                try:
                    error_msg = response.json().get('message', response.text)
                except:
                    error_msg = response.text
                log(f"âŒ è¿æ¥å¤±è´¥ ({response.status_code}): {error_msg}", "ERROR")
                return False
        except Exception as e:
            log(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {e}", "ERROR")
            return False

    def get_file_id(self, filename: str, remote_path: str = "/") -> Optional[int]:
        """
        é€šè¿‡æ–‡ä»¶åå’Œè·¯å¾„è·å–æ–‡ä»¶ID
        
        å‚æ•°:
            filename: æ–‡ä»¶å
            remote_path: è¿œç¨‹è·¯å¾„ï¼ˆç”¨äºç²¾ç¡®åŒ¹é…ï¼Œé¿å…åŒåæ–‡ä»¶å†²çªï¼‰
        """
        url = f"{self.BASE_URL}/projects/{self.project_id}/files"
        try:
            response = self._request_with_retry("GET", url)
            if response.status_code != 200:
                log(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {response.text}", "ERROR")
                return None
                
            files = response.json()
            log(f"é¡¹ç›®ä¸­å…±æœ‰ {len(files)} ä¸ªæ–‡ä»¶")
            
            # æ„å»ºå®Œæ•´è·¯å¾„è¿›è¡ŒåŒ¹é…
            # Paratranz æ–‡ä»¶çš„ name å­—æ®µåŒ…å«å®Œæ•´è·¯å¾„ï¼Œå¦‚ "Entities/Clothing.json"
            expected_full_path = (remote_path.strip('/') + '/' + filename).lstrip('/')
            
            for f in files:
                file_name = f.get('name', '')
                # å…ˆå°è¯•å®Œæ•´è·¯å¾„åŒ¹é…
                if file_name == expected_full_path:
                    log(f"æ‰¾åˆ°æ–‡ä»¶ (å®Œæ•´è·¯å¾„): {expected_full_path} (ID: {f.get('id')})")
                    return f.get('id')
            
            # å›é€€ï¼šä»…æŒ‰æ–‡ä»¶ååŒ¹é…ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
            for f in files:
                file_name = f.get('name', '')
                if file_name == filename or file_name.endswith('/' + filename):
                    log(f"æ‰¾åˆ°æ–‡ä»¶ (æ–‡ä»¶ååŒ¹é…): {file_name} (ID: {f.get('id')})")
                    return f.get('id')
            
            log(f"æœªæ‰¾åˆ°æ–‡ä»¶: {expected_full_path}", "WARNING")
            return None
        except Exception as e:
            log(f"è·å–æ–‡ä»¶åˆ—è¡¨é”™è¯¯: {e}", "ERROR")
            return None

    def upload_file(self, file_path: str, remote_path: str = "/") -> bool:
        """ä¸Šä¼ æ–‡ä»¶åˆ° Paratranzï¼ˆåˆ›å»ºæˆ–æ›´æ–°ï¼‰"""
        if not os.path.exists(file_path):
            log(f"âŒ æœªæ‰¾åˆ°æœ¬åœ°æ–‡ä»¶: {file_path}", "ERROR")
            return False

        filename = os.path.basename(file_path)
        log(f"å‡†å¤‡ä¸Šä¼ æ–‡ä»¶: {filename} -> {remote_path}")
        
        # ä¼ é€’ remote_path è¿›è¡Œç²¾ç¡®åŒ¹é…
        file_id = self.get_file_id(filename, remote_path)

        try:
            with open(file_path, 'rb') as f:
                files = {'file': (filename, f, 'application/json')}
                
                if file_id:
                    log(f"æ›´æ–°ç°æœ‰æ–‡ä»¶: {filename} (ID: {file_id})")
                    url = f"{self.BASE_URL}/projects/{self.project_id}/files/{file_id}"
                    response = self._request_with_retry("POST", url, files=files)
                else:
                    log(f"åˆ›å»ºæ–°æ–‡ä»¶: {filename} åœ¨ {remote_path}")
                    url = f"{self.BASE_URL}/projects/{self.project_id}/files"
                    data = {'path': remote_path}
                    response = self._request_with_retry("POST", url, files=files, data=data)

            if response.status_code in [200, 201]:
                log("âœ… ä¸Šä¼ æˆåŠŸ")
                return True
            else:
                try:
                    error_msg = response.json().get('message', response.text)
                except:
                    error_msg = response.text
                log(f"âŒ ä¸Šä¼ å¤±è´¥: {error_msg}", "ERROR")
                return False
        except Exception as e:
            log(f"âŒ ä¸Šä¼ å¼‚å¸¸: {e}", "ERROR")
            return False

    def upload_folder(self, local_dir: str) -> Dict[str, int]:
        """
        æ‰¹é‡ä¸Šä¼ ç›®å½•ä¸‹æ‰€æœ‰ JSON æ–‡ä»¶åˆ° Paratranzï¼Œä¿æŒè·¯å¾„ç»“æ„ã€‚
        
        æ–‡ä»¶åæ ¼å¼çº¦å®šï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
        1. å®Œæ•´è·¯å¾„æ ¼å¼ï¼šEntities/Clothing.json -> ä¸Šä¼ åˆ° /Entities/Clothing/ è·¯å¾„
        2. ä¸‹åˆ’çº¿æ ¼å¼ï¼ˆæ—§ç‰ˆå…¼å®¹ï¼‰ï¼šEntities_Clothing.json -> ä¸Šä¼ åˆ° /Entities/Clothing/ è·¯å¾„
        
        è¿”å›ç»Ÿè®¡ä¿¡æ¯ã€‚
        """
        stats = {"uploaded": 0, "failed": 0, "skipped": 0}
        
        if not os.path.isdir(local_dir):
            log(f"âŒ ç›®å½•ä¸å­˜åœ¨: {local_dir}", "ERROR")
            return stats
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ JSON æ–‡ä»¶ï¼ˆæ”¯æŒå­ç›®å½•ç»“æ„ï¼‰
        json_files = []
        for root, dirs, files in os.walk(local_dir):
            for f in files:
                if f.endswith('.json'):
                    # ä¿å­˜ç›¸å¯¹è·¯å¾„
                    rel_path = os.path.relpath(os.path.join(root, f), local_dir)
                    json_files.append(rel_path)
        
        if not json_files:
            log(f"âš ï¸ ç›®å½•ä¸­æ²¡æœ‰ JSON æ–‡ä»¶: {local_dir}", "WARNING")
            return stats
        
        log(f"ğŸ“¤ æ‰¹é‡ä¸Šä¼  {len(json_files)} ä¸ªæ–‡ä»¶åˆ° Paratranz...")
        
        for i, rel_path in enumerate(json_files):
            file_path = os.path.join(local_dir, rel_path)
            
            # ä»ç›¸å¯¹è·¯å¾„æ¨æ–­è¿œç¨‹è·¯å¾„
            # ä¾‹å¦‚ï¼šEntities/Clothing.json -> /Entities/Clothing/
            # æˆ–è€…ï¼šEntities/Clothing/Hats.json -> /Entities/Clothing/Hats/
            remote_dir = os.path.dirname(rel_path).replace('\\', '/')
            base_name = os.path.splitext(os.path.basename(rel_path))[0]
            
            if remote_dir:
                # æœ‰å­ç›®å½•ï¼šEntities/Clothing/Hats.json -> /Entities/Clothing/Hats/
                remote_path = '/' + remote_dir + '/' + base_name + '/'
            else:
                # æ ¹ç›®å½•ä¸‹ï¼šEntities.json -> /Entities/ æˆ– Entities/Clothing.json -> /Entities/Clothing/
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼ˆå®Œæ•´è·¯å¾„æ ¼å¼ï¼‰
                if '/' in base_name:
                    remote_path = '/' + base_name + '/'
                else:
                    remote_path = '/' + base_name + '/'
            
            # æ¸…ç†å¤šä½™çš„æ–œæ 
            remote_path = '/' + remote_path.strip('/').replace('//', '/') + '/'
            if remote_path == '//':
                remote_path = '/'
            
            log(f"[{i+1}/{len(json_files)}] ä¸Šä¼ : {rel_path} -> {remote_path}")
            
            if self.upload_file(file_path, remote_path):
                stats["uploaded"] += 1
            else:
                stats["failed"] += 1
            
            # é¿å… API é€Ÿç‡é™åˆ¶
            time.sleep(0.5)
        
        log(f"âœ… æ‰¹é‡ä¸Šä¼ å®Œæˆã€‚æˆåŠŸ: {stats['uploaded']}ï¼Œå¤±è´¥: {stats['failed']}")
        return stats

    def trigger_export(self) -> bool:
        """è§¦å‘é¡¹ç›®å¯¼å‡ºï¼ˆç”Ÿæˆå‹ç¼©åŒ…ï¼‰"""
        log("è§¦å‘é¡¹ç›®å¯¼å‡º...")
        url = f"{self.BASE_URL}/projects/{self.project_id}/artifacts"
        try:
            response = self._request_with_retry("POST", url)
            if response.status_code in [200, 201]:
                log("âœ… å¯¼å‡ºä»»åŠ¡å·²è§¦å‘")
                return True
            elif response.status_code == 403:
                log("âš ï¸ æ²¡æœ‰è§¦å‘å¯¼å‡ºçš„æƒé™ï¼ˆä»…ç®¡ç†å‘˜å¯ç”¨ï¼‰ï¼Œå°†å°è¯•ä¸‹è½½ç°æœ‰å¯¼å‡º", "WARNING")
                return True  # ç»§ç»­å°è¯•ä¸‹è½½
            else:
                try:
                    error_msg = response.json().get('message', response.text)
                except:
                    error_msg = response.text
                log(f"è§¦å‘å¯¼å‡ºå¤±è´¥: {error_msg}", "WARNING")
                return True  # ç»§ç»­å°è¯•ä¸‹è½½ç°æœ‰å¯¼å‡º
        except Exception as e:
            log(f"è§¦å‘å¯¼å‡ºå¼‚å¸¸: {e}", "WARNING")
            return True  # ç»§ç»­å°è¯•ä¸‹è½½

    def download_artifacts(self, save_path: str) -> bool:
        """
        ä¸‹è½½é¡¹ç›®å¯¼å‡ºçš„å‹ç¼©åŒ…ï¼ˆåŒ…å«æ‰€æœ‰ç¿»è¯‘ï¼‰
        æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼šGET /projects/{projectId}/artifacts/download
        """
        log("æ­£åœ¨ä¸‹è½½ç¿»è¯‘å‹ç¼©åŒ…...")
        
        # å…ˆå°è¯•è§¦å‘å¯¼å‡ºï¼ˆå¯èƒ½éœ€è¦ç®¡ç†å‘˜æƒé™ï¼‰
        self.trigger_export()
        
        # ç­‰å¾…ä¸€å°ä¼šè®©æœåŠ¡å™¨å‡†å¤‡
        time.sleep(1)
        
        url = f"{self.BASE_URL}/projects/{self.project_id}/artifacts/download"
        
        try:
            # ä½¿ç”¨ allow_redirects=True è·Ÿéšé‡å®šå‘
            log(f"è¯·æ±‚ä¸‹è½½: {url}")
            response = requests.get(url, headers=self.headers, timeout=60, allow_redirects=True, stream=True)
            
            log(f"ä¸‹è½½å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                # ä¿å­˜ä¸º zip æ–‡ä»¶
                zip_path = save_path.replace('.json', '.zip') if save_path.endswith('.json') else save_path + '.zip'
                with open(zip_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                log(f"âœ… å·²ä¸‹è½½å‹ç¼©åŒ…åˆ°: {zip_path}")
                
                # è§£å‹å¹¶æå–ç¿»è¯‘
                return self._extract_translations_from_zip(zip_path, save_path)
                
            elif response.status_code == 302:
                # æ‰‹åŠ¨å¤„ç†é‡å®šå‘
                redirect_url = response.headers.get('Location')
                if redirect_url:
                    log(f"é‡å®šå‘åˆ°: {redirect_url}")
                    response = requests.get(redirect_url, timeout=60, stream=True)
                    if response.status_code == 200:
                        zip_path = save_path.replace('.json', '.zip') if save_path.endswith('.json') else save_path + '.zip'
                        with open(zip_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                        log(f"âœ… å·²ä¸‹è½½å‹ç¼©åŒ…åˆ°: {zip_path}")
                        return self._extract_translations_from_zip(zip_path, save_path)
            else:
                try:
                    error_msg = response.json().get('message', response.text[:200])
                except:
                    error_msg = response.text[:200] if response.text else "æœªçŸ¥é”™è¯¯"
                log(f"âŒ ä¸‹è½½å¤±è´¥ ({response.status_code}): {error_msg}", "ERROR")
                return False
                
        except Exception as e:
            log(f"âŒ ä¸‹è½½å¼‚å¸¸: {e}", "ERROR")
            return False

    def _extract_translations_from_zip(self, zip_path: str, output_path: str) -> bool:
        """ä»ä¸‹è½½çš„å‹ç¼©åŒ…ä¸­æå–ç¿»è¯‘æ•°æ®"""
        import zipfile
        
        try:
            log(f"æ­£åœ¨è§£å‹: {zip_path}")
            
            with zipfile.ZipFile(zip_path, 'r') as zf:
                # åˆ—å‡ºå‹ç¼©åŒ…å†…å®¹
                file_list = zf.namelist()
                log(f"å‹ç¼©åŒ…å†…åŒ…å« {len(file_list)} ä¸ªæ–‡ä»¶")
                
                # æŸ¥æ‰¾ JSON æ–‡ä»¶
                json_files = [f for f in file_list if f.endswith('.json')]
                
                if json_files:
                    # æå–ç¬¬ä¸€ä¸ª JSON æ–‡ä»¶
                    target_file = json_files[0]
                    log(f"æå–æ–‡ä»¶: {target_file}")
                    
                    with zf.open(target_file) as source:
                        content = source.read()
                        with open(output_path, 'wb') as target:
                            target.write(content)
                    
                    log(f"âœ… å·²ä¿å­˜ç¿»è¯‘åˆ°: {output_path}")
                    return True
                else:
                    # å¦‚æœæ²¡æœ‰ JSONï¼Œè§£å‹æ‰€æœ‰æ–‡ä»¶åˆ°å½“å‰ç›®å½•
                    extract_dir = os.path.dirname(output_path) or '.'
                    zf.extractall(extract_dir)
                    log(f"âœ… å·²è§£å‹æ‰€æœ‰æ–‡ä»¶åˆ°: {extract_dir}")
                    return True
                    
        except zipfile.BadZipFile:
            log("âŒ ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„å‹ç¼©åŒ…", "ERROR")
            return False
        except Exception as e:
            log(f"âŒ è§£å‹å¤±è´¥: {e}", "ERROR")
            return False
        finally:
            # æ¸…ç†ä¸´æ—¶ zip æ–‡ä»¶
            try:
                if os.path.exists(zip_path):
                    os.remove(zip_path)
            except:
                pass

    def download_file(self, save_path: str, remote_filename: str = "") -> bool:
        """
        ä¸‹è½½ç¿»è¯‘æ–‡ä»¶
        ä½¿ç”¨ artifacts/download ç«¯ç‚¹ä¸‹è½½é¡¹ç›®å¯¼å‡º
        """
        return self.download_artifacts(save_path)

# ==================== æå–é€»è¾‘ (Extraction Logic) ====================

# å“ˆå¸Œç¼“å­˜æ–‡ä»¶å
HASH_CACHE_FILE = ".extract_cache.json"

def compute_file_hash(file_path: str) -> str:
    """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼"""
    hasher = hashlib.md5()
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            hasher.update(chunk)
    return hasher.hexdigest()

def load_hash_cache(cache_file: str) -> Dict[str, str]:
    """åŠ è½½å“ˆå¸Œç¼“å­˜"""
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {}

def save_hash_cache(cache_file: str, cache: Dict[str, str]):
    """ä¿å­˜å“ˆå¸Œç¼“å­˜"""
    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(cache, f, indent=2)

def extract_strings(scan_dir: str, output_file: str, fields: List[str] = None, 
                    incremental: bool = False, filter_ftl: bool = True) -> Dict[str, int]:
    """
    æ‰«æ scan_dir ä¸­çš„ YAML æ–‡ä»¶å¹¶æå–å­—ç¬¦ä¸²åˆ° output_fileã€‚
    
    å‚æ•°:
        scan_dir: æ‰«æç›®å½•
        output_file: è¾“å‡º JSON æ–‡ä»¶
        fields: è¦æå–çš„å­—æ®µåˆ—è¡¨
        incremental: æ˜¯å¦ä½¿ç”¨å¢é‡æ¨¡å¼ï¼ˆè·³è¿‡æœªä¿®æ”¹çš„æ–‡ä»¶ï¼‰
        filter_ftl: æ˜¯å¦è¿‡æ»¤ FTL æœ¬åœ°åŒ–é”®ï¼ˆé»˜è®¤å¼€å¯ï¼‰
    
    è¿”å›ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    if fields is None:
        fields = DEFAULT_TRANSLATABLE_FIELDS
        
    yaml_processor = YAMLProcessor()
    extracted_data: List[Dict] = []
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "files_scanned": 0,
        "files_with_text": 0,
        "files_skipped": 0,  # å¢é‡æ¨¡å¼ä¸‹è·³è¿‡çš„æ–‡ä»¶æ•°
        "ftl_skipped": 0,    # FTL é”®è¿‡æ»¤è·³è¿‡çš„æ¡ç›®æ•°
        "total_strings": 0,
        "by_field": {f: 0 for f in fields}
    }
    
    # å¢é‡æ¨¡å¼ï¼šåŠ è½½å“ˆå¸Œç¼“å­˜å’Œå·²æœ‰æå–ç»“æœ
    cache_file = os.path.join(os.path.dirname(output_file) or '.', HASH_CACHE_FILE)
    hash_cache: Dict[str, str] = {}
    existing_data: Dict[str, Dict] = {}  # key -> entry
    
    if incremental:
        log("ğŸ“Š å¢é‡æ¨¡å¼å·²å¯ç”¨ï¼Œå°†è·³è¿‡æœªä¿®æ”¹çš„æ–‡ä»¶")
        hash_cache = load_hash_cache(cache_file)
        
        # åŠ è½½å·²æœ‰çš„æå–ç»“æœ
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    for entry in json.load(f):
                        existing_data[entry.get('key', '')] = entry
                log(f"å·²åŠ è½½ {len(existing_data)} æ¡å·²æœ‰æå–è®°å½•")
            except:
                pass
    
    new_hash_cache: Dict[str, str] = {}
    
    # æ”¶é›†æ‰€æœ‰ YAML æ–‡ä»¶
    yaml_files = []
    for root, dirs, files in os.walk(scan_dir):
        for file in files:
            if file.endswith('.yml') or file.endswith('.yaml'):
                yaml_files.append(os.path.join(root, file))
    
    total_files = len(yaml_files)
    log(f"æ­£åœ¨æ‰«æç›®å½•: {scan_dir} (å…± {total_files} ä¸ªæ–‡ä»¶)")
    
    for i, file_path in enumerate(yaml_files):
        rel_path = os.path.relpath(file_path, scan_dir)
        stats["files_scanned"] += 1
        report_progress(i + 1, total_files, f"æ‰«æ: {rel_path}")
        
        # å¢é‡æ¨¡å¼ï¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
        if incremental:
            current_hash = compute_file_hash(file_path)
            new_hash_cache[rel_path] = current_hash
            
            if rel_path in hash_cache and hash_cache[rel_path] == current_hash:
                # æ–‡ä»¶æœªä¿®æ”¹ï¼Œè·³è¿‡æ‰«æï¼Œä½†ä¿ç•™å·²æœ‰æ•°æ®
                stats["files_skipped"] += 1
                continue
        
        try:
            data = yaml_processor.load(file_path)
            if not data:
                continue
            
            count_before = len(extracted_data)
            
            if isinstance(data, list):
                for node in data:
                    process_node_extract(node, extracted_data, rel_path, fields, stats, filter_ftl)
            elif isinstance(data, dict):
                process_node_extract(data, extracted_data, rel_path, fields, stats, filter_ftl)
            
            if len(extracted_data) > count_before:
                stats["files_with_text"] += 1
                 
        except Exception as e:
            log(f"è§£ææ–‡ä»¶é”™è¯¯ {file_path}: {e}", "WARNING")

    # å¢é‡æ¨¡å¼ï¼šåˆå¹¶å·²æœ‰æ•°æ®å’Œæ–°æ•°æ®
    if incremental and existing_data:
        # æ„å»ºæ–°æ•°æ®çš„ key é›†åˆ
        new_keys = {entry['key'] for entry in extracted_data}
        
        # ä¿ç•™æœªè¢«é‡æ–°æ‰«æçš„æ–‡ä»¶ä¸­çš„æ•°æ®
        for key, entry in existing_data.items():
            if key not in new_keys:
                # æ£€æŸ¥è¯¥æ¡ç›®å¯¹åº”çš„æ–‡ä»¶æ˜¯å¦è¢«è·³è¿‡ï¼ˆä»ç„¶å­˜åœ¨ï¼‰
                context = entry.get('context', '')
                # ä» context ä¸­æå–æ–‡ä»¶è·¯å¾„
                if 'æ–‡ä»¶:' in context:
                    file_line = context.split('\n')[0]
                    file_path = file_line.replace('æ–‡ä»¶:', '').strip()
                    if file_path in new_hash_cache:
                        extracted_data.append(entry)
        
        log(f"å¢é‡åˆå¹¶åå…± {len(extracted_data)} æ¡è®°å½•")

    stats["total_strings"] = len(extracted_data)
    
    log(f"æå–å®Œæˆã€‚æ‰«æ {stats['files_scanned']} ä¸ªæ–‡ä»¶ï¼Œ"
        f"è·³è¿‡ {stats['files_skipped']} ä¸ªï¼ˆæœªä¿®æ”¹ï¼‰ï¼Œ"
        f"å…¶ä¸­ {stats['files_with_text']} ä¸ªåŒ…å«æ–‡æœ¬ï¼Œ"
        f"å…± {stats['total_strings']} æ¡å­—ç¬¦ä¸²ã€‚")
    
    if stats.get("ftl_skipped", 0) > 0:
        log(f"  âš ï¸ å·²è¿‡æ»¤ {stats['ftl_skipped']} æ¡ FTL æœ¬åœ°åŒ–é”®")
    
    # è¾“å‡ºå­—æ®µç»Ÿè®¡
    for field, count in stats["by_field"].items():
        if count > 0:
            log(f"  - {field}: {count} æ¡")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(extracted_data, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æ–°çš„å“ˆå¸Œç¼“å­˜
    if incremental:
        save_hash_cache(cache_file, new_hash_cache)
        log(f"å·²æ›´æ–°æ–‡ä»¶å“ˆå¸Œç¼“å­˜: {cache_file}")
    
    log(f"å·²ä¿å­˜åˆ° {output_file}")
    return stats

def process_node_extract(node: Dict, extracted_data: List, rel_path: str, 
                         fields: List[str], stats: Dict, filter_ftl: bool = True):
    """
    æå–å•ä¸ªèŠ‚ç‚¹ä¸­çš„æ–‡æœ¬ã€‚
    
    å‚æ•°:
        filter_ftl: æ˜¯å¦è¿‡æ»¤ FTL æœ¬åœ°åŒ–é”®ï¼ˆé»˜è®¤å¼€å¯ï¼‰
    """
    if not isinstance(node, dict):
        return
        
    entity_type = node.get('type')
    entity_id = node.get('id')
    parent = node.get('parent')
    
    if entity_type == 'entity' or 'id' in node:
        if entity_id:
            key_prefix = entity_id
        elif parent:
            p_str = parent[0] if isinstance(parent, list) and parent else str(parent)
            key_prefix = f"Parent_{p_str}"
            if node.get('suffix'):
                key_prefix += f"_{node.get('suffix')}"
        else:
            return 

        for field in fields:
            if field in node and isinstance(node[field], str):
                original_text = node[field]
                if not original_text.strip():
                    continue
                
                # FTL é”®è¿‡æ»¤ï¼šè·³è¿‡å½¢å¦‚ "loadout-group-weapon" çš„æœ¬åœ°åŒ–å¼•ç”¨
                if filter_ftl and is_ftl_key(original_text):
                    stats["ftl_skipped"] = stats.get("ftl_skipped", 0) + 1
                    continue
                
                key = f"{key_prefix}.{field}"
                
                context = f"æ–‡ä»¶: {rel_path}\n"
                if entity_id:
                    context += f"ID: {entity_id}\n"
                if parent:
                    context += f"Parent: {parent}\n"
                
                extracted_data.append({
                    "key": key,
                    "original": original_text,
                    "context": context
                })
                
                stats["by_field"][field] = stats["by_field"].get(field, 0) + 1

# ==================== æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„æå– (Folder-Based Extraction) ====================

from collections import defaultdict
import glob

def extract_strings_by_folder(scan_dir: str, output_dir: str, fields: List[str] = None,
                               filter_ftl: bool = True) -> Dict[str, int]:
    """
    æŒ‰æ–‡ä»¶å¤¹ç»“æ„æå–ï¼Œæ¯ä¸ªæ–‡ä»¶å¤¹ç”Ÿæˆä¸€ä¸ª JSON æ–‡ä»¶ï¼Œä¿ç•™å®Œæ•´ç›®å½•ç»“æ„ã€‚
    
    å‚æ•°:
        scan_dir: æ‰«æç›®å½•
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå­˜æ”¾å¤šä¸ª JSON æ–‡ä»¶ï¼‰
        fields: è¦æå–çš„å­—æ®µåˆ—è¡¨
        filter_ftl: æ˜¯å¦è¿‡æ»¤ FTL æœ¬åœ°åŒ–é”®
    
    è¿”å›ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    if fields is None:
        fields = DEFAULT_TRANSLATABLE_FIELDS
    
    yaml_processor = YAMLProcessor()
    
    # æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„çš„æ•°æ®
    folder_data: Dict[str, List[Dict]] = defaultdict(list)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "files_scanned": 0,
        "files_with_text": 0,
        "ftl_skipped": 0,
        "total_strings": 0,
        "folder_count": 0,
        "by_field": {f: 0 for f in fields}
    }
    
    # æ”¶é›†æ‰€æœ‰ YAML æ–‡ä»¶
    yaml_files = []
    for root, dirs, files in os.walk(scan_dir):
        for file in files:
            if file.endswith('.yml') or file.endswith('.yaml'):
                yaml_files.append(os.path.join(root, file))
    
    total_files = len(yaml_files)
    log(f"ğŸ“‚ æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„æ¨¡å¼ï¼šæ‰«æ {scan_dir} (å…± {total_files} ä¸ªæ–‡ä»¶)")
    
    for i, file_path in enumerate(yaml_files):
        rel_path = os.path.relpath(file_path, scan_dir)
        stats["files_scanned"] += 1
        report_progress(i + 1, total_files, f"æ‰«æ: {rel_path}")
        
        # è®¡ç®—åˆ†ç»„é”®ï¼ˆä½¿ç”¨å®Œæ•´æ–‡ä»¶å¤¹è·¯å¾„ï¼Œç”¨ / åˆ†éš”ï¼‰
        folder = os.path.dirname(rel_path).replace('\\', '/')
        
        # ä½¿ç”¨å®Œæ•´è·¯å¾„ä½œä¸ºåˆ†ç»„é”®ï¼Œç©ºè·¯å¾„ä½¿ç”¨ "root"
        group_key = folder if folder else "root"
        
        try:
            data = yaml_processor.load(file_path)
            if not data:
                continue
            
            # ä¸´æ—¶å­˜å‚¨æœ¬æ–‡ä»¶æå–çš„æ•°æ®
            file_entries: List[Dict] = []
            file_stats = {"ftl_skipped": 0, "by_field": {f: 0 for f in fields}}
            
            if isinstance(data, list):
                for node in data:
                    process_node_extract(node, file_entries, rel_path, fields, file_stats, filter_ftl)
            elif isinstance(data, dict):
                process_node_extract(data, file_entries, rel_path, fields, file_stats, filter_ftl)
            
            if file_entries:
                folder_data[group_key].extend(file_entries)
                stats["files_with_text"] += 1
            
            # ç´¯è®¡ç»Ÿè®¡
            stats["ftl_skipped"] += file_stats.get("ftl_skipped", 0)
            for field, count in file_stats["by_field"].items():
                stats["by_field"][field] = stats["by_field"].get(field, 0) + count
                
        except Exception as e:
            log(f"è§£ææ–‡ä»¶é”™è¯¯ {file_path}: {e}", "WARNING")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¸ºæ¯ä¸ªåˆ†ç»„ç”Ÿæˆ JSON æ–‡ä»¶ï¼ˆä¿æŒç›®å½•ç»“æ„ï¼‰
    for group_name, entries in folder_data.items():
        if not entries:
            continue
        
        # group_name æ ¼å¼å¦‚ "Entities/Clothing" æˆ– "root"
        if group_name == "root":
            output_file = os.path.join(output_dir, "root.json")
        else:
            # åˆ›å»ºå­ç›®å½•ç»“æ„
            # Entities/Clothing -> output_dir/Entities/Clothing.json
            parent_dir = os.path.dirname(group_name)
            base_name = os.path.basename(group_name)
            
            if parent_dir:
                full_parent = os.path.join(output_dir, parent_dir)
                os.makedirs(full_parent, exist_ok=True)
                output_file = os.path.join(full_parent, f"{base_name}.json")
            else:
                output_file = os.path.join(output_dir, f"{base_name}.json")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(entries, f, indent=2, ensure_ascii=False)
        
        stats["total_strings"] += len(entries)
        stats["folder_count"] += 1
        log(f"  ğŸ“„ {group_name}.json: {len(entries)} æ¡")
    
    log(f"âœ… æå–å®Œæˆã€‚æ‰«æ {stats['files_scanned']} ä¸ªæ–‡ä»¶ï¼Œ"
        f"ç”Ÿæˆ {stats['folder_count']} ä¸ª JSON æ–‡ä»¶ï¼Œ"
        f"å…± {stats['total_strings']} æ¡å­—ç¬¦ä¸²ã€‚")
    
    if stats.get("ftl_skipped", 0) > 0:
        log(f"  âš ï¸ å·²è¿‡æ»¤ {stats['ftl_skipped']} æ¡ FTL æœ¬åœ°åŒ–é”®")
    
    return stats

# ==================== åˆå¹¶é€»è¾‘ (Merge Logic) ====================

def merge_translations(repo_root: str, translation_file: str, output_dir: str, 
                       fields: List[str] = None) -> Dict[str, int]:
    """
    å°† JSON ä¸­çš„ç¿»è¯‘åˆå¹¶åˆ° repo_root ä¸‹çš„ YAML æ–‡ä»¶ä¸­ã€‚
    è¿”å›ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    if fields is None:
        fields = DEFAULT_TRANSLATABLE_FIELDS
        
    yaml_processor = YAMLProcessor()
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "files_modified": 0,
        "strings_applied": 0,
        "strings_skipped": 0,
        "translations_unused": 0
    }
    
    if not os.path.exists(translation_file):
        log(f"æœªæ‰¾åˆ°ç¿»è¯‘æ–‡ä»¶: {translation_file}", "ERROR")
        return stats

    with open(translation_file, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
        
    translation_map: Dict[str, str] = {}
    if isinstance(raw_data, list):
        for item in raw_data:
            start_key = item.get('key')
            translation = item.get('translation', '')
            if start_key and translation:
                translation_map[start_key] = translation
    elif isinstance(raw_data, dict):
        translation_map = raw_data
    
    used_keys = set()
    log(f"å·²åŠ è½½ {len(translation_map)} æ¡ç¿»è¯‘ã€‚")
    
    # æ”¶é›†æ‰€æœ‰ YAML æ–‡ä»¶
    yaml_files = []
    for root, dirs, files in os.walk(repo_root):
        for file in files:
            if file.endswith('.yml') or file.endswith('.yaml'):
                yaml_files.append(os.path.join(root, file))
    
    total_files = len(yaml_files)
    log(f"è¯»å–ç›®å½•: {repo_root} (å…± {total_files} ä¸ªæ–‡ä»¶)")
    log(f"å†™å…¥ç›®å½•: {output_dir}")
    
    for i, file_path in enumerate(yaml_files):
        rel_path = os.path.relpath(file_path, repo_root)
        report_progress(i + 1, total_files, f"åˆå¹¶: {rel_path}")
        
        try:
            data = yaml_processor.load(file_path)
            if not data:
                continue
            
            modified = False
            
            if isinstance(data, list):
                for node in data:
                    result = process_node_merge(node, translation_map, fields, used_keys)
                    if result["modified"]:
                        modified = True
                    stats["strings_applied"] += result["applied"]
                    stats["strings_skipped"] += result["skipped"]
            elif isinstance(data, dict):
                result = process_node_merge(data, translation_map, fields, used_keys)
                if result["modified"]:
                    modified = True
                stats["strings_applied"] += result["applied"]
                stats["strings_skipped"] += result["skipped"]
            
            if modified:
                out_path = os.path.join(output_dir, rel_path)
                os.makedirs(os.path.dirname(out_path), exist_ok=True)
                yaml_processor.dump(data, out_path)
                stats["files_modified"] += 1
                
        except Exception as e:
            log(f"å¤„ç†æ–‡ä»¶é”™è¯¯ {file_path}: {e}", "WARNING")
    
    stats["translations_unused"] = len(translation_map) - len(used_keys)
    
    log(f"åˆå¹¶å®Œæˆã€‚ä¿®æ”¹äº† {stats['files_modified']} ä¸ªæ–‡ä»¶ï¼Œ"
        f"åº”ç”¨äº† {stats['strings_applied']} æ¡ç¿»è¯‘ï¼Œ"
        f"è·³è¿‡ {stats['strings_skipped']} æ¡ï¼ˆå†…å®¹ç›¸åŒï¼‰ï¼Œ"
        f"æœªä½¿ç”¨ {stats['translations_unused']} æ¡ã€‚")
    
    return stats

def process_node_merge(node: Dict, translation_map: Dict[str, str], 
                       fields: List[str], used_keys: set) -> Dict[str, Any]:
    """åˆå¹¶å•ä¸ªèŠ‚ç‚¹ä¸­çš„ç¿»è¯‘"""
    result = {"modified": False, "applied": 0, "skipped": 0}
    
    if not isinstance(node, dict):
        return result
        
    entity_id = node.get('id')
    parent = node.get('parent')
    
    if 'id' in node or node.get('type') == 'entity':
        if entity_id:
            key_prefix = entity_id
        elif parent:
            p_str = parent[0] if isinstance(parent, list) and parent else str(parent)
            key_prefix = f"Parent_{p_str}"
            if node.get('suffix'):
                key_prefix += f"_{node.get('suffix')}"
        else:
            return result

        for field in fields:
            if field in node:
                key = f"{key_prefix}.{field}"
                if key in translation_map:
                    used_keys.add(key)
                    new_text = translation_map[key]
                    if new_text and new_text != node[field]:
                        node[field] = new_text
                        result["modified"] = True
                        result["applied"] += 1
                    else:
                        result["skipped"] += 1
                         
    return result

# ==================== ä¸€é”®å·¥ä½œæµ (One-Click Workflow) ====================

def run_full_workflow(scan_dir: str, output_json: str, translation_json: str,
                      project_id: int, token: str, output_dir: str,
                      fields: List[str] = None, by_folder: bool = False,
                      filter_ftl: bool = True, incremental: bool = False) -> bool:
    """
    æ‰§è¡Œå®Œæ•´çš„æ±‰åŒ–å·¥ä½œæµï¼šæå– â†’ ä¸Šä¼  â†’ ä¸‹è½½ â†’ åˆå¹¶
    
    å‚æ•°:
        by_folder: æ˜¯å¦æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„æ¨¡å¼ï¼ˆç”Ÿæˆå¤šä¸ª JSONï¼‰
        filter_ftl: æ˜¯å¦è¿‡æ»¤ FTL é”®
    """
    log("=" * 50)
    log("å¼€å§‹æ‰§è¡Œä¸€é”®å·¥ä½œæµ")
    if by_folder:
        log("ğŸ“‚ æ¨¡å¼: æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„")
    else:
        log("ğŸ“„ æ¨¡å¼: å•æ–‡ä»¶")
    log("=" * 50)
    
    # æ­¥éª¤ 1: æå–
    log("\n[æ­¥éª¤ 1/4] æå–åŸæ–‡...")
    try:
        if by_folder:
            # åˆ†ç»„æ¨¡å¼ï¼šè¾“å‡ºåˆ°ç›®å½•
            extract_strings_by_folder(scan_dir, output_json, fields, 
                                      filter_ftl=filter_ftl)
        else:
            # å•æ–‡ä»¶æ¨¡å¼
            extract_strings(scan_dir, output_json, fields, 
                           incremental=incremental, filter_ftl=filter_ftl)
    except Exception as e:
        log(f"æå–å¤±è´¥: {e}", "ERROR")
        return False
    
    # æ­¥éª¤ 2: ä¸Šä¼ 
    log("\n[æ­¥éª¤ 2/4] ä¸Šä¼ åˆ° Paratranz...")
    try:
        client = PZClient(project_id, token)
        if not client.test_connection():
            log("API è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œç»ˆæ­¢å·¥ä½œæµ", "ERROR")
            return False
        
        if by_folder:
            # åˆ†ç»„æ¨¡å¼ï¼šæ‰¹é‡ä¸Šä¼ ç›®å½•
            result = client.upload_folder(output_json)
            if result.get("failed", 0) > result.get("uploaded", 0):
                log("æ‰¹é‡ä¸Šä¼ å¤±è´¥è¿‡å¤šï¼Œç»ˆæ­¢å·¥ä½œæµ", "ERROR")
                return False
        else:
            # å•æ–‡ä»¶æ¨¡å¼
            if not client.upload_file(output_json):
                log("ä¸Šä¼ å¤±è´¥ï¼Œç»ˆæ­¢å·¥ä½œæµ", "ERROR")
                return False
    except Exception as e:
        log(f"ä¸Šä¼ å¤±è´¥: {e}", "ERROR")
        return False
    
    # æ­¥éª¤ 3: ä¸‹è½½
    log("\n[æ­¥éª¤ 3/4] ä» Paratranz ä¸‹è½½ç¿»è¯‘...")
    try:
        if not client.download_file(translation_json, os.path.basename(output_json)):
            log("ä¸‹è½½å¤±è´¥ï¼Œç»ˆæ­¢å·¥ä½œæµ", "ERROR")
            return False
    except Exception as e:
        log(f"ä¸‹è½½å¤±è´¥: {e}", "ERROR")
        return False
    
    # æ­¥éª¤ 4: åˆå¹¶
    log("\n[æ­¥éª¤ 4/4] åˆå¹¶ç¿»è¯‘...")
    try:
        merge_translations(scan_dir, translation_json, output_dir, fields)
    except Exception as e:
        log(f"åˆå¹¶å¤±è´¥: {e}", "ERROR")
        return False
    
    log("\n" + "=" * 50)
    log("âœ… ä¸€é”®å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼")
    log("=" * 50)
    return True

# ==================== å›¾å½¢ç•Œé¢ (GUI) ====================

import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext

class App:
    def __init__(self, root):
        self.root = root
        self.root.title("SS14 è‡ªåŠ¨åŒ–æ±‰åŒ–å·¥å…·ç®± v3.0")
        self.root.geometry("850x700")
        
        style = ttk.Style()
        style.theme_use('clam')
        
        self.config = self.load_config()
        self.is_running = False
        
        main_frame = ttk.Frame(root, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # æ ‡é¢˜
        title_label = ttk.Label(main_frame, text="SS14 æ±‰åŒ–å·¥ä½œæµå·¥å…· v3.2", 
                                font=("Microsoft YaHei UI", 16, "bold"))
        title_label.pack(pady=(0, 10))
        
        # è¿›åº¦æ¡åŒºåŸŸ
        progress_frame = ttk.Frame(main_frame)
        progress_frame.pack(fill=tk.X, pady=(0, 10))
        
        self.progress_var = tk.DoubleVar()
        self.progress_bar = ttk.Progressbar(progress_frame, variable=self.progress_var, 
                                            maximum=100, mode='determinate')
        self.progress_bar.pack(fill=tk.X, side=tk.LEFT, expand=True)
        
        self.progress_label = ttk.Label(progress_frame, text="å°±ç»ª", width=30)
        self.progress_label.pack(side=tk.RIGHT, padx=(10, 0))
        
        # ä¸€é”®å·¥ä½œæµæŒ‰é’®ï¼ˆçªå‡ºæ˜¾ç¤ºï¼‰
        workflow_frame = ttk.LabelFrame(main_frame, text="ğŸš€ ä¸€é”®å·¥ä½œæµï¼ˆæ¨èï¼‰", padding=10)
        workflow_frame.pack(fill=tk.X, pady=(0, 10))
        
        ttk.Label(workflow_frame, text="è‡ªåŠ¨æ‰§è¡Œ: æå–åŸæ–‡ â†’ ä¸Šä¼  Paratranz â†’ ä¸‹è½½ç¿»è¯‘ â†’ åˆå¹¶å›æ¸¸æˆæ–‡ä»¶",
                  foreground="gray").pack(side=tk.LEFT)
        
        self.btn_workflow = ttk.Button(workflow_frame, text="âš¡ å¼€å§‹ä¸€é”®åŒæ­¥", 
                                        command=self.do_full_workflow)
        self.btn_workflow.pack(side=tk.RIGHT, ipadx=20)
        
        # é€‰é¡¹å¡
        self.notebook = ttk.Notebook(main_frame)
        self.notebook.pack(fill=tk.BOTH, expand=True)
        
        self.tab_extract = ttk.Frame(self.notebook, padding=10)
        self.notebook.add(self.tab_extract, text="1. æå–åŸæ–‡")
        self.setup_extract_tab()
        
        self.tab_sync = ttk.Frame(self.notebook, padding=10)
        self.notebook.add(self.tab_sync, text="2. Paratranz åŒæ­¥")
        self.setup_sync_tab()
        
        self.tab_merge = ttk.Frame(self.notebook, padding=10)
        self.notebook.add(self.tab_merge, text="3. åˆå¹¶ç¿»è¯‘")
        self.setup_merge_tab()
        
        self.tab_settings = ttk.Frame(self.notebook, padding=10)
        self.notebook.add(self.tab_settings, text="âš™ï¸ è®¾ç½®")
        self.setup_settings_tab()
        
        # æ—¥å¿—åŒºåŸŸ
        log_frame = ttk.LabelFrame(main_frame, text="è¿è¡Œæ—¥å¿—", padding=5)
        log_frame.pack(fill=tk.BOTH, expand=True, pady=(10, 0))
        
        self.log_text = scrolledtext.ScrolledText(log_frame, height=12, state='disabled', 
                                                  font=("Consolas", 9))
        self.log_text.pack(fill=tk.BOTH, expand=True)
        
        # çŠ¶æ€æ 
        self.status_var = tk.StringVar()
        self.status_var.set("å°±ç»ª")
        status_bar = ttk.Label(root, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)
        status_bar.pack(side=tk.BOTTOM, fill=tk.X)

        self.root.protocol("WM_DELETE_WINDOW", self.on_close)
        
        # è‡ªåŠ¨æ£€æµ‹ç›®å½•
        self.auto_detect_directory()

    def auto_detect_directory(self):
        """å¯åŠ¨æ—¶è‡ªåŠ¨æ£€æµ‹æ¸¸æˆç›®å½•"""
        detected = detect_game_directory()
        if detected:
            self.extract_dir_var.set(detected)
            self.merge_source_var.set(detected)
            self.log(f"è‡ªåŠ¨æ£€æµ‹åˆ°æ¸¸æˆç›®å½•: {detected}")

    def load_config(self) -> Dict:
        default_config = {
            "extract_dir": "Resources/Prototypes",
            "extract_output": "en.json",
            "pz_token": "",
            "pz_project_id": "16648",
            "merge_source": "Resources/Prototypes",
            "merge_input": "zh.json",
            "download_path": "zh.json",  # æ–°å¢ï¼šä¸‹è½½ä¿å­˜è·¯å¾„
            "translatable_fields": DEFAULT_TRANSLATABLE_FIELDS
        }
        if os.path.exists(CONFIG_FILE):
            try:
                with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                    return {**default_config, **json.load(f)}
            except:
                return default_config
        return default_config

    def save_config(self):
        self.config["extract_dir"] = self.extract_dir_var.get()
        self.config["extract_output"] = self.extract_output_var.get()
        self.config["pz_token"] = self.pz_token_var.get()
        self.config["pz_project_id"] = self.pz_project_id_var.get()
        self.config["merge_source"] = self.merge_source_var.get()
        self.config["merge_input"] = self.merge_input_var.get()
        self.config["download_path"] = self.download_path_var.get()
        self.config["translatable_fields"] = [f.strip() for f in self.fields_var.get().split(',')]
        # æ–°å¢é€‰é¡¹
        self.config["filter_ftl"] = self.filter_ftl_var.get()
        self.config["by_folder"] = self.by_folder_var.get()
        self.config["folder_depth"] = int(self.folder_depth_var.get())
        
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=4, ensure_ascii=False)

    def on_close(self):
        self.save_config()
        self.root.destroy()

    def log(self, message: str):
        def _log():
            self.log_text.config(state='normal')
            self.log_text.insert(tk.END, message + "\n")
            self.log_text.see(tk.END)
            self.log_text.config(state='disabled')
        self.root.after(0, _log)

    def update_progress(self, current: int, total: int, message: str = ""):
        def _update():
            if total > 0:
                percent = (current / total) * 100
                self.progress_var.set(percent)
                self.progress_label.config(text=f"{current}/{total} - {message[:30]}")
        self.root.after(0, _update)

    def run_in_thread(self, func, *args, success_msg="æ“ä½œæˆåŠŸå®Œæˆï¼"):
        """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œå‡½æ•°"""
        if self.is_running:
            messagebox.showwarning("æç¤º", "æœ‰æ“ä½œæ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç¨å€™...")
            return
            
        def _run():
            self.is_running = True
            self.status_var.set("æ­£åœ¨è¿è¡Œ...")
            self.progress_var.set(0)
            
            # è®¾ç½®è¿›åº¦å›è°ƒ
            set_progress_callback(self.update_progress)
            
            try:
                result = func(*args)
                
                if result is False:
                    self.log("âŒ æ“ä½œå¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šæ–¹æ—¥å¿—ã€‚")
                    self.status_var.set("æ“ä½œå¤±è´¥")
                    self.root.after(0, lambda: messagebox.showerror("é”™è¯¯", "æ“ä½œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—ã€‚"))
                else:
                    self.log(f"âœ… {success_msg}")
                    self.status_var.set("æ“ä½œæˆåŠŸ")
                    self.progress_var.set(100)
                    self.root.after(0, lambda: messagebox.showinfo("æˆåŠŸ", success_msg))
                    
            except Exception as e:
                self.log(f"âŒ å‘ç”Ÿå¼‚å¸¸: {str(e)}")
                self.status_var.set("å‘ç”Ÿé”™è¯¯")
                self.root.after(0, lambda: messagebox.showerror("é”™è¯¯", f"è¿è¡Œå¼‚å¸¸: {e}"))
            finally:
                self.is_running = False
                set_progress_callback(None)

        threading.Thread(target=_run, daemon=True).start()

    # ===== ç•Œé¢è®¾ç½® =====

    def setup_extract_tab(self):
        frame = self.tab_extract
        ttk.Label(frame, text="æ‰«æç›®å½•:").grid(row=0, column=0, sticky='w', pady=5)
        self.extract_dir_var = tk.StringVar(value=self.config["extract_dir"])
        ttk.Entry(frame, textvariable=self.extract_dir_var, width=50).grid(row=0, column=1, padx=5)
        ttk.Button(frame, text="æµè§ˆ...", command=lambda: self.select_folder(self.extract_dir_var)).grid(row=0, column=2)
        
        ttk.Label(frame, text="è¾“å‡ºæ–‡ä»¶/ç›®å½•:").grid(row=1, column=0, sticky='w', pady=5)
        self.extract_output_var = tk.StringVar(value=self.config["extract_output"])
        ttk.Entry(frame, textvariable=self.extract_output_var, width=50).grid(row=1, column=1, padx=5)
        ttk.Button(frame, text="æµè§ˆ...", command=self._select_extract_output).grid(row=1, column=2)
        
        # é€‰é¡¹åŒºåŸŸ
        options_frame = ttk.LabelFrame(frame, text="æå–é€‰é¡¹", padding=10)
        options_frame.grid(row=2, column=0, columnspan=3, sticky='ew', pady=10, padx=5)
        
        # FTL è¿‡æ»¤é€‰é¡¹
        self.filter_ftl_var = tk.BooleanVar(value=self.config.get("filter_ftl", True))
        ttk.Checkbutton(options_frame, text="è¿‡æ»¤ FTL æœ¬åœ°åŒ–é”®å€¼ (è·³è¿‡ loadout-group-weapon ç±»)", 
                       variable=self.filter_ftl_var).grid(row=0, column=0, sticky='w')
        
        # æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„é€‰é¡¹
        self.by_folder_var = tk.BooleanVar(value=self.config.get("by_folder", False))
        ttk.Checkbutton(options_frame, text="æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„æå– (ä¿ç•™å®Œæ•´ç›®å½•ç»“æ„ï¼Œç”Ÿæˆå¤šä¸ª JSON)", 
                       variable=self.by_folder_var).grid(row=1, column=0, sticky='w')
        
        ttk.Button(frame, text="å¼€å§‹æå–", command=self.do_extract).grid(row=3, column=1, pady=10, ipadx=20)

    def _select_extract_output(self):
        """é€‰æ‹©æå–è¾“å‡ºä½ç½®ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰"""
        if self.by_folder_var.get():
            # æŒ‰æ–‡ä»¶å¤¹æ¨¡å¼ï¼šé€‰æ‹©ç›®å½•
            folder = filedialog.askdirectory(title="é€‰æ‹©è¾“å‡ºç›®å½•")
            if folder:
                self.extract_output_var.set(folder)
        else:
            # å•æ–‡ä»¶æ¨¡å¼ï¼šé€‰æ‹©æ–‡ä»¶
            file = filedialog.asksaveasfilename(
                defaultextension=".json",
                filetypes=[("JSON files", "*.json"), ("All files", "*.*")],
                initialfile="en.json"
            )
            if file:
                self.extract_output_var.set(file)

    def setup_sync_tab(self):
        frame = self.tab_sync
        ttk.Label(frame, text="é¡¹ç›® ID:").grid(row=0, column=0, sticky='w', pady=5)
        self.pz_project_id_var = tk.StringVar(value=self.config["pz_project_id"])
        ttk.Entry(frame, textvariable=self.pz_project_id_var, width=20).grid(row=0, column=1, sticky='w', padx=5)
        
        ttk.Label(frame, text="API Token:").grid(row=1, column=0, sticky='w', pady=5)
        self.pz_token_var = tk.StringVar(value=self.config["pz_token"])
        ttk.Entry(frame, textvariable=self.pz_token_var, width=50, show="*").grid(row=1, column=1, padx=5)
        
        # ä¸‹è½½ä½ç½®
        ttk.Label(frame, text="ä¸‹è½½ä¿å­˜è·¯å¾„:").grid(row=2, column=0, sticky='w', pady=5)
        self.download_path_var = tk.StringVar(value=self.config.get("download_path", "zh.json"))
        ttk.Entry(frame, textvariable=self.download_path_var, width=50).grid(row=2, column=1, padx=5)
        ttk.Button(frame, text="æµè§ˆ...", command=self._select_download_path).grid(row=2, column=2)
        
        btn_frame = ttk.Frame(frame)
        btn_frame.grid(row=3, column=0, columnspan=3, pady=20)
        
        ttk.Button(btn_frame, text="æµ‹è¯•è¿æ¥", command=self.do_test_connection).pack(side=tk.LEFT, padx=5)
        ttk.Button(btn_frame, text="â¬†ï¸ ä¸Šä¼ ", command=self.do_upload).pack(side=tk.LEFT, padx=5, ipadx=10)
        ttk.Button(btn_frame, text="â¬‡ï¸ ä¸‹è½½", command=self.do_download).pack(side=tk.LEFT, padx=5, ipadx=10)

    def _select_download_path(self):
        """é€‰æ‹©ä¸‹è½½ä¿å­˜ä½ç½®"""
        file = filedialog.asksaveasfilename(
            defaultextension=".json",
            filetypes=[("JSON files", "*.json"), ("All files", "*.*")],
            initialfile="zh.json"
        )
        if file:
            self.download_path_var.set(file)

    def setup_merge_tab(self):
        frame = self.tab_merge
        ttk.Label(frame, text="ç¿»è¯‘æ–‡ä»¶:").grid(row=0, column=0, sticky='w', pady=5)
        self.merge_input_var = tk.StringVar(value=self.config["merge_input"])
        ttk.Entry(frame, textvariable=self.merge_input_var, width=50).grid(row=0, column=1, padx=5)
        ttk.Button(frame, text="æµè§ˆ...", command=lambda: self.select_file(self.merge_input_var)).grid(row=0, column=2)
        
        ttk.Label(frame, text="ç›®æ ‡ç›®å½•:").grid(row=1, column=0, sticky='w', pady=5)
        self.merge_source_var = tk.StringVar(value=self.config["merge_source"])
        ttk.Entry(frame, textvariable=self.merge_source_var, width=50).grid(row=1, column=1, padx=5)
        ttk.Button(frame, text="æµè§ˆ...", command=lambda: self.select_folder(self.merge_source_var)).grid(row=1, column=2)
        
        ttk.Label(frame, text="æ³¨æ„ï¼šæ“ä½œå‰å»ºè®®å¤‡ä»½ç›®æ ‡ç›®å½•ã€‚", foreground="#d9534f").grid(row=2, column=0, columnspan=3, pady=10)
        
        ttk.Button(frame, text="å¼€å§‹åˆå¹¶", command=self.do_merge).grid(row=3, column=1, pady=10, ipadx=20)

    def setup_settings_tab(self):
        frame = self.tab_settings
        
        ttk.Label(frame, text="å¯ç¿»è¯‘å­—æ®µ (é€—å·åˆ†éš”):").grid(row=0, column=0, sticky='w', pady=5)
        fields_str = ', '.join(self.config.get("translatable_fields", DEFAULT_TRANSLATABLE_FIELDS))
        self.fields_var = tk.StringVar(value=fields_str)
        ttk.Entry(frame, textvariable=self.fields_var, width=50).grid(row=0, column=1, padx=5)
        
        ttk.Label(frame, text="é»˜è®¤: name, description", foreground="gray").grid(row=1, column=1, sticky='w')
        
        ttk.Button(frame, text="ä¿å­˜è®¾ç½®", command=self.save_config).grid(row=2, column=1, pady=20)

    def select_folder(self, var):
        folder = filedialog.askdirectory()
        if folder:
            var.set(folder)

    def select_file(self, var):
        file = filedialog.askopenfilename(filetypes=[("JSON files", "*.json"), ("All files", "*.*")])
        if file:
            var.set(file)

    # ===== æ“ä½œå‡½æ•° =====

    def get_fields(self) -> List[str]:
        return [f.strip() for f in self.fields_var.get().split(',') if f.strip()]

    def do_extract(self):
        target = self.extract_dir_var.get()
        output = self.extract_output_var.get()
        if not target or not output:
            messagebox.showwarning("æç¤º", "è¯·å¡«å†™ç›®å½•å’Œè¾“å‡ºæ–‡ä»¶/ç›®å½•")
            return
        
        filter_ftl = self.filter_ftl_var.get()
        by_folder = self.by_folder_var.get()
        fields = self.get_fields()
        
        if by_folder:
            # æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„æ¨¡å¼
            self.run_in_thread(extract_strings_by_folder, target, output, fields, 
                              filter_ftl,
                              success_msg="æå–å®Œæˆï¼")
        else:
            # å•æ–‡ä»¶æ¨¡å¼
            self.run_in_thread(extract_strings, target, output, fields, 
                              False, filter_ftl,  # incremental=False, filter_ftl
                              success_msg="æå–å®Œæˆï¼")

    def do_test_connection(self):
        token = self.pz_token_var.get()
        pid = self.pz_project_id_var.get()
        if not token or not pid:
            messagebox.showwarning("æç¤º", "è¯·å¡«å†™é¡¹ç›® ID å’Œ Token")
            return
        
        def _test():
            client = PZClient(int(pid), token)
            return client.test_connection()
        
        self.run_in_thread(_test, success_msg="è¿æ¥æµ‹è¯•æˆåŠŸï¼")

    def do_upload(self):
        token = self.pz_token_var.get()
        pid = self.pz_project_id_var.get()
        target = self.extract_output_var.get()
        if not token:
            messagebox.showwarning("æç¤º", "è¯·è¾“å…¥ Token")
            return
        if not target:
            messagebox.showwarning("æç¤º", "è¯·æŒ‡å®šè¦ä¸Šä¼ çš„æ–‡ä»¶æˆ–ç›®å½•")
            return
        
        def _upload():
            client = PZClient(int(pid), token)
            # è‡ªåŠ¨æ£€æµ‹ï¼šå¦‚æœæ˜¯ç›®å½•åˆ™æ‰¹é‡ä¸Šä¼ ï¼Œå¦åˆ™ä¸Šä¼ å•æ–‡ä»¶
            if os.path.isdir(target):
                self.log(f"ğŸ“‚ æ£€æµ‹åˆ°ç›®å½•ï¼Œä½¿ç”¨æ‰¹é‡ä¸Šä¼ æ¨¡å¼: {target}")
                result = client.upload_folder(target)
                return result.get("uploaded", 0) > 0 or result.get("failed", 0) == 0
            else:
                return client.upload_file(target)
        
        self.run_in_thread(_upload, success_msg="ä¸Šä¼ æˆåŠŸï¼")

    def do_download(self):
        token = self.pz_token_var.get()
        pid = self.pz_project_id_var.get()
        local_file = self.download_path_var.get()  # ä½¿ç”¨æ–°çš„ä¸‹è½½è·¯å¾„å˜é‡
        if not token:
            messagebox.showwarning("æç¤º", "è¯·è¾“å…¥ Token")
            return
        if not local_file:
            messagebox.showwarning("æç¤º", "è¯·æŒ‡å®šä¸‹è½½ä¿å­˜è·¯å¾„")
            return
        
        def _download():
            client = PZClient(int(pid), token)
            return client.download_file(local_file)
        
        self.run_in_thread(_download, success_msg=f"ä¸‹è½½æˆåŠŸï¼å·²ä¿å­˜åˆ°: {local_file}")

    def do_merge(self):
        input_file = self.merge_input_var.get()
        target_dir = self.merge_source_var.get()
        if not os.path.exists(input_file):
            messagebox.showwarning("é”™è¯¯", f"æ‰¾ä¸åˆ°ç¿»è¯‘æ–‡ä»¶ï¼š{input_file}")
            return
        self.run_in_thread(merge_translations, target_dir, input_file, target_dir, self.get_fields(),
                          success_msg="åˆå¹¶å®Œæˆï¼")

    def do_full_workflow(self):
        """æ‰§è¡Œä¸€é”®å·¥ä½œæµ"""
        token = self.pz_token_var.get()
        pid = self.pz_project_id_var.get()
        
        if not token:
            messagebox.showwarning("æç¤º", "è¯·å…ˆåœ¨ã€ŒParatranz åŒæ­¥ã€é€‰é¡¹å¡ä¸­å¡«å†™ API Token")
            self.notebook.select(self.tab_sync)
            return
        
        scan_dir = self.extract_dir_var.get()
        output_json = self.extract_output_var.get()
        translation_json = self.merge_input_var.get()
        output_dir = self.merge_source_var.get()
        fields = self.get_fields()
        
        # è¯»å–åˆ†ç»„æ¨¡å¼é€‰é¡¹
        by_folder = self.by_folder_var.get()
        filter_ftl = self.filter_ftl_var.get()
        
        self.run_in_thread(
            run_full_workflow,
            scan_dir, output_json, translation_json,
            int(pid), token, output_dir, fields,
            by_folder, filter_ftl,
            success_msg="ä¸€é”®å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼"
        )

# ==================== ä¸»ç¨‹åºå…¥å£ (Main) ====================

if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œå¯åŠ¨ GUI
    if len(sys.argv) == 1:
        try:
            root = tk.Tk()
            app = App(root)
            root.mainloop()
        except Exception as e:
            with open("error_log.txt", "w") as f:
                import traceback
                f.write(traceback.format_exc())
    # å¦åˆ™è§£æå‘½ä»¤è¡Œå‚æ•°
    else:
        parser = argparse.ArgumentParser(description="SS14 Localization Tracker CLI v3.0")
        subparsers = parser.add_subparsers(dest='command')
        
        # Extract å‘½ä»¤
        p_extract = subparsers.add_parser('extract', help='Extract strings')
        p_extract.add_argument('--source', help='Source repo root (Optional)')
        p_extract.add_argument('--target_folders', required=True, help='Target directory')
        p_extract.add_argument('--output', required=True, help='Output JSON file or directory (for --by-folder)')
        p_extract.add_argument('--fields', help='Comma-separated list of fields to extract')
        p_extract.add_argument('--incremental', action='store_true', 
                               help='Enable incremental mode (skip unchanged files)')
        p_extract.add_argument('--filter-ftl', action='store_true', default=True,
                               help='Filter out FTL localization keys (default: enabled)')
        p_extract.add_argument('--no-filter-ftl', action='store_true',
                               help='Disable FTL key filtering')
        p_extract.add_argument('--by-folder', action='store_true',
                               help='Generate multiple JSON files by folder structure')
        
        # Merge å‘½ä»¤
        p_merge = subparsers.add_parser('merge', help='Merge translations')
        p_merge.add_argument('--source', required=True, help='Source directory')
        p_merge.add_argument('--input', required=True, help='Input JSON file')
        p_merge.add_argument('--output', required=True, help='Output directory')
        p_merge.add_argument('--fields', help='Comma-separated list of fields to merge')
        
        # Upload å‘½ä»¤
        p_upload = subparsers.add_parser('upload', help='Upload to Paratranz')
        p_upload.add_argument('--file', help='Single file to upload')
        p_upload.add_argument('--folder', help='Folder of JSON files to upload (batch mode)')
        p_upload.add_argument('--project_id', type=int, default=os.environ.get('PZ_PROJECT_ID'))
        p_upload.add_argument('--token', default=os.environ.get('PARATRANZ_TOKEN'))
        
        # Download å‘½ä»¤
        p_download = subparsers.add_parser('download', help='Download from Paratranz')
        p_download.add_argument('--file', required=True, help='File to save')
        p_download.add_argument('--remote', help='Remote filename match')
        p_download.add_argument('--project_id', type=int, default=os.environ.get('PZ_PROJECT_ID'))
        p_download.add_argument('--token', default=os.environ.get('PARATRANZ_TOKEN'))
        
        # Workflow å‘½ä»¤
        p_workflow = subparsers.add_parser('workflow', help='Run full workflow')
        p_workflow.add_argument('--scan_dir', required=True, help='Directory to scan')
        p_workflow.add_argument('--output_json', default='en.json', help='Extracted JSON file')
        p_workflow.add_argument('--translation_json', default='zh.json', help='Translation JSON file')
        p_workflow.add_argument('--output_dir', required=True, help='Output directory for merged files')
        p_workflow.add_argument('--project_id', type=int, default=os.environ.get('PZ_PROJECT_ID'))
        p_workflow.add_argument('--token', default=os.environ.get('PARATRANZ_TOKEN'))
        p_workflow.add_argument('--incremental', action='store_true', 
                               help='Enable incremental extraction mode')
        p_workflow.add_argument('--by-folder', action='store_true',
                               help='Generate multiple JSON files by folder structure')
        p_workflow.add_argument('--filter-ftl', action='store_true', default=True,
                               help='Filter out FTL localization keys (default: enabled)')
        p_workflow.add_argument('--no-filter-ftl', action='store_true',
                               help='Disable FTL key filtering')
        
        args = parser.parse_args()
        
        if args.command == 'extract':
            scan_path = args.target_folders
            if args.source:
                scan_path = os.path.join(args.source, args.target_folders)
            fields = args.fields.split(',') if args.fields else None
            filter_ftl = not args.no_filter_ftl  # é»˜è®¤å¼€å¯è¿‡æ»¤
            
            if args.by_folder:
                # æŒ‰æ–‡ä»¶å¤¹åˆ†ç»„æ¨¡å¼
                extract_strings_by_folder(scan_path, args.output, fields, 
                                          filter_ftl=filter_ftl)
            else:
                # å•æ–‡ä»¶æ¨¡å¼
                extract_strings(scan_path, args.output, fields, 
                               incremental=args.incremental, filter_ftl=filter_ftl)
            
        elif args.command == 'merge':
            src_path = args.source
            if os.path.exists(os.path.join(src_path, 'Content')):
                src_path = os.path.join(src_path, 'Content')
            fields = args.fields.split(',') if args.fields else None
            merge_translations(src_path, args.input, args.output, fields)
            
        elif args.command == 'upload':
            client = PZClient(args.project_id, args.token)
            if args.folder:
                # æ‰¹é‡ä¸Šä¼ æ¨¡å¼
                client.upload_folder(args.folder)
            elif args.file:
                client.upload_file(args.file)
            else:
                log("è¯·æŒ‡å®š --file æˆ– --folder å‚æ•°", "ERROR")
            
        elif args.command == 'download':
            client = PZClient(args.project_id, args.token)
            client.download_file(args.file, args.remote)
            
        elif args.command == 'workflow':
            filter_ftl = not args.no_filter_ftl
            run_full_workflow(
                args.scan_dir, args.output_json, args.translation_json,
                args.project_id, args.token, args.output_dir,
                incremental=args.incremental, by_folder=args.by_folder,
                filter_ftl=filter_ftl
            )

